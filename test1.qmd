---
title: "Pokémon Card Market Analysis"
subtitle: "Index Behavior, Predictability & Risk Factors"
author: "Your Name"
date: today
format:
  html:
    theme: cosmo
    toc: true
    toc-depth: 3
    code-fold: true
    number-sections: true
execute:
  warning: false
  message: false
---

```{r setup}
#| label: setup

library(tidyverse)
library(tidymodels)
library(tidyquant)
library(lubridate)
library(zoo)
library(car)
library(rpart.plot)
library(gt)
library(broom)

set.seed(42)
```

# Executive Summary

This analysis investigates whether a portfolio of ~40 Pokémon trading cards exhibits market-like behavior and explores predictability using two complementary methodologies:

1. **Recursive Regression Method (Ch. 8)** — Systematic feature selection: *which variables* to include
2. **Tidymodels ML Workflow (Ch. 11)** — Model training & evaluation: *how to estimate & compare models*

**Key Questions:**

1. **Index Behavior**: Does an aggregated basket behave like a market index?
2. **Basket Prediction**: Can we predict next-day basket movements?
3. **Card-Level Risk**: What card characteristics explain volatility differences?
4. **Individual Backtest**: Does the basket model generalize to individual cards?

---

# Data Loading & Cleaning

```{r load-data}
#| label: load-data

parse_card_file <- function(filepath, card_id) {
  all_lines <- readLines(filepath, warn = FALSE)
  card_info_start <- which(grepl("CARD INFORMATION", all_lines))[1]
  history_start <- which(grepl("PRICE HISTORY", all_lines))[1]
  card_lines <- all_lines[(card_info_start + 1):(history_start - 3)]
  card_lines <- card_lines[card_lines != ""]
  
  card_name <- str_extract(card_lines[grepl("Card Name", card_lines)], "(?<=Card Name,).*")
  set_name <- str_extract(card_lines[grepl("Set Name", card_lines)], "(?<=Set Name,).*")
  rarity <- str_extract(card_lines[grepl("Rarity", card_lines)], "(?<=Rarity,).*")
  
  price_history_raw <- read.csv(
    text = paste(all_lines[(history_start + 1):length(all_lines)], collapse = "\n"),
    header = TRUE, stringsAsFactors = FALSE
  )
  
  price_history_raw %>%
    select(1:5) %>%
    rename(Date = 1, Condition = 2, Price = 3, Volume = 4, DataType = 5) %>%
    mutate(
      Date = as.Date(substr(as.character(Date), 1, 10)),
      Price = as.numeric(gsub("[\\$,]", "", Price)),
      Volume = as.numeric(Volume),
      DataType = trimws(DataType),
      card_id = card_id,
      card_name = trimws(card_name),
      set_name = trimws(set_name),
      rarity = trimws(rarity)
    ) %>%
    filter(!is.na(Price), !is.na(Date))
}

# file_path <- "C:/Users/kenne/Downloads/Pokemon Project/"
# file_path <- "C:/Users/kenne/Downloads/Csvs from sept 2023 - cleaned/"
file_path <- "C:/Users/kenne/Downloads/Csvs from sept 2023/"
csv_files <- list.files(
  path = file_path,
  pattern = "\\.csv$",        # Only files ending in .csv
  full.names = TRUE           # Get full file paths
)

cat("Found", length(csv_files), "CSV files\n")

# Parse each file
df_list <- list()
for (i in seq_along(csv_files)) {
  filename <- csv_files[i]
  cat("Loading:", basename(filename), "\n")
  
  # Use the index i as card_id
  df_list[[i]] <- parse_card_file(filename, card_id = i)
}

df_raw <- bind_rows(df_list)
```

## Data Quality Check

```{r data-quality}
#| label: data-quality

df_nm <- df_raw %>% filter(Condition == "Near Mint")

data_quality <- df_nm %>%
  group_by(DataType) %>%
  summarise(count = n(), pct = round(n() / nrow(df_nm) * 100, 1)) %>%
  arrange(desc(count))

cat("=== DATA QUALITY CHECK ===\n\n")
print(data_quality)

volume_check <- df_nm %>%
  summarise(
    total_obs = n(),
    zero_volume = sum(Volume == 0, na.rm = TRUE),
    pct_zero_vol = round(zero_volume / total_obs * 100, 1),
    has_volume = sum(Volume > 0, na.rm = TRUE)
  )

cat("\n=== VOLUME CHECK ===\n")
cat("Zero volume:", volume_check$zero_volume, "(", volume_check$pct_zero_vol, "%)\n")
cat("Has volume:", volume_check$has_volume, "\n")
```

## Filter to Actual Trades

```{r filter-actual-trades}
#| label: filter-actual-trades

df <- df_nm %>%
  filter(Volume > 0 | DataType != "Interpolated") %>%
  arrange(card_id, Date) %>%
  group_by(card_id) %>%
  mutate(
    daily_return = (Price - lag(Price)) / lag(Price),
    log_return = log(Price / lag(Price))
  ) %>%
  ungroup() %>%
  filter(!is.na(daily_return))

cat("\n=== AFTER FILTERING ===\n")
cat("Observations:", nrow(df), "\n")
cat("Unique cards:", n_distinct(df$card_id), "\n")
```

---

# Part 1: Does the Basket Behave Like a Market Index?

A true market index typically displays:

- **Fat tails** (kurtosis > 3): More extreme events than normal distribution predicts
- **Negative skewness**: Crashes larger than rallies
- **Volatility clustering**: Calm periods followed by turbulent periods

```{r construct-index}
#| label: construct-index

basket_index <- df %>%
  group_by(Date) %>%
  summarise(
    total_value = sum(Price, na.rm = TRUE),
    basket_return = sum((Price / total_value) * daily_return, na.rm = TRUE),
    equal_weight_return = mean(daily_return, na.rm = TRUE),
    n_cards = n_distinct(card_id),
    total_volume = sum(Volume, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(n_cards >= 5) %>%
  arrange(Date) %>%
  mutate(
    basket_level = cumprod(1 + replace_na(basket_return, 0)),
    ew_level = cumprod(1 + replace_na(equal_weight_return, 0))
  )

cat("Basket observations:", nrow(basket_index), "\n")
```

```{r plot-index}
#| label: plot-index

ggplot(basket_index, aes(x = Date)) +
  geom_line(aes(y = basket_level, color = "Value-Weighted"), linewidth = 1) +
  geom_line(aes(y = ew_level, color = "Equal-Weighted"), linewidth = 0.8, alpha = 0.7) +
  geom_hline(yintercept = 1, linetype = "dashed", alpha = 0.5) +
  scale_color_manual(values = c("Value-Weighted" = "#e74c3c", "Equal-Weighted" = "#3498db")) +
  labs(title = "Pokémon Card Basket Index (Actual Trades Only)", x = "Date", y = "Index Level") +
  theme_minimal() + theme(legend.position = "bottom")
```

## Statistical Properties

```{r index-stats}
#| label: index-stats

returns <- basket_index$basket_return

tibble(
  Statistic = c("Mean (daily)", "Std Dev", "Ann. Vol", "Skewness", "Kurtosis", "Min", "Max"),
  Value = round(c(
    mean(returns, na.rm = TRUE),
    sd(returns, na.rm = TRUE),
    sd(returns, na.rm = TRUE) * sqrt(252),
    e1071::skewness(returns, na.rm = TRUE),
    e1071::kurtosis(returns, na.rm = TRUE),
    min(returns, na.rm = TRUE),
    max(returns, na.rm = TRUE)
  ), 4)
) %>% gt() %>% tab_header(title = "Basket Return Statistics")
```

## Autocorrelation Analysis

```{r autocorrelation}
#| label: autocorrelation

par(mfrow = c(1, 2))
acf(returns, lag.max = 20, main = "Return ACF", na.action = na.pass)
acf(returns^2, lag.max = 20, main = "Squared Returns ACF\n(Volatility Clustering)", na.action = na.pass)
par(mfrow = c(1, 1))
```

**Commentary:**

- **Return ACF**: Bars within blue bands = returns largely unpredictable from past
- **Squared Returns ACF**: Significant autocorrelation = volatility clustering

## Part 1 Verdict

```{r verdict-1}
#| label: verdict-1

kurt <- e1071::kurtosis(returns, na.rm = TRUE)
skew <- e1071::skewness(returns, na.rm = TRUE)
ann_vol <- sd(returns, na.rm = TRUE) * sqrt(252)

tibble(
  Property = c("Fat Tails (Kurtosis > 3)", "Negative Skewness", "High Volatility (>20%)"),
  Expected = c("Yes", "Yes", "Yes"),
  Observed = c(
    ifelse(kurt > 3, paste0("✓ ", round(kurt, 2)), paste0("✗ ", round(kurt, 2))),
    ifelse(skew < 0, paste0("✓ ", round(skew, 2)), paste0("✗ ", round(skew, 2))),
    ifelse(ann_vol > 0.20, paste0("✓ ", round(ann_vol*100, 1), "%"), paste0("✗ ", round(ann_vol*100, 1), "%"))
  )
) %>%
  gt() %>%
  tab_header(title = "Does the Basket Behave Like a Market Index?")
```

---

# Part 2: Predictability Analysis

**Main question :** Can we predict the Pokemon basket's next-day return using historical patterns and macroeconomic factors?

In order to do this, we ran a Recursive Regression to find the most impactful variable, and feed into the ML workflow in order to find the in and out-of-sample fit.

## 2.1 Feature Engineering

We build the feature based on the following classification of assumptions : 

**1. Momentum Features (Hypothesis: Past returns predict future returns)**

Features Created:

- `return_lag1, return_lag2, return_lag3` - Recent 1-3 day returns
- `return_5d` - Cumulative 5-day momentum

The assumption is that momentum exist and because of the inefficient nature of the Pokemon market, prices adjust slowly to new information.  

**2. Volatility Features (Hypothesis: Recent volatility predicts future returns)**

Features Created:

- `vol_5d` - 5-day rolling standard deviation of returns

The assumption is that recent volatility is a proxy for “risk & excitement” in the market .We assume that a higher recent volatility change might be systematically linked to next-day returns, even if the sign is ambiguous.

```{r building the features, include = FALSE}
consumer_sentiment <- tidyquant::tq_get(
  "UMCSENT", get = "economic.data",
  from = min(basket_index$Date) - days(90), to = max(basket_index$Date)
) %>% rename(Date = date, sentiment_index = price)

spy_data <- tidyquant::tq_get(
  "SPY", get = "stock.prices",
  from = min(basket_index$Date) - days(30), to = max(basket_index$Date)
) %>%
  mutate(spy_return = (adjusted - lag(adjusted)) / lag(adjusted)) %>%
  rename(Date = date) %>%
  select(Date, spy_return)

# Create daily aligned versions
all_dates <- tibble(Date = seq(min(basket_index$Date), max(basket_index$Date), by = "day"))

sentiment_daily <- tibble(Date = seq(min(consumer_sentiment$Date), max(all_dates$Date), by = "day")) %>%
  left_join(consumer_sentiment, by = "Date") %>%
  tidyr::fill(sentiment_index, .direction = "downup")

spy_daily <- all_dates %>%
  left_join(spy_data, by = "Date") %>%
  mutate(spy_return = replace_na(spy_return, 0))

basket_features <- basket_index %>%
  left_join(sentiment_daily, by = "Date") %>%
  left_join(spy_daily, by = "Date") %>%
  mutate(
    return_next = lead(basket_return),
    return_lag1 = lag(basket_return, 1),
    return_lag2 = lag(basket_return, 2),
    return_lag3 = lag(basket_return, 3),
    return_5d = dplyr::lag(
      slider::slide_dbl(
        basket_return,
        ~ sum(.x),         
        .before   = 4,      
        .complete = TRUE    
      ),
      1                     
    ),
    vol_5d = dplyr::lag(
      slider::slide_dbl(
        basket_return,
        ~ stats::sd(.x),    
        .before   = 4,
        .complete = TRUE
      ),
      1
    ),
    sentiment_lag1 = lag(sentiment_index, 1),
    spy_lag1 = lag(spy_return, 1),
    spy_5d = dplyr::lag(
      slider::slide_dbl(
        spy_return,
        ~ sum(.x),
        .before   = 4,
        .complete = TRUE
      ),
      1
    ),
    dow = wday(Date, label = TRUE),
    is_holiday_season = ifelse(month(Date) %in% c(11, 12), 1, 0)
  ) %>%
  filter(!is.na(return_next), !is.na(vol_5d), !is.na(spy_lag1))

cat("Feature set:", nrow(basket_features), "observations\n")
```

---

## 2.2 Recursive Regression: Feature Selection

Following the recursive method from class (Ch. 8), we systematically identify which predictors to include.

### Step 1: Univariate Screening

```{r univariate-screening}
#| label: univariate-screening

predictors <- c("return_lag1", "return_lag2", "return_lag3", "return_5d", 
                "vol_5d", "sentiment_lag1", "spy_lag1", "spy_5d", "is_holiday_season")

bf_clean <- basket_features %>% 
  dplyr::select(Date, return_next, c(predictors)) %>% 
  tidyr::drop_na()

univariate_results <- bf_clean %>% 
  dplyr::select(-Date, -return_next) %>%  
  purrr::map(~ lm(bf_clean$return_next ~ .x, data = bf_clean)) %>% 
  purrr::map(summary) %>% 
  purrr::map_dbl("r.squared") %>% 
  tibble::tibble(Predictor = names(.), R_squared = .) %>% 
  dplyr::arrange(dplyr::desc(R_squared))

univariate_results %>%
  gt() %>%
  fmt_percent(columns = R_squared, decimals = 2) %>%
  tab_header(title = "Step 1: Univariate Screening", 
             subtitle = "Predictors ranked by standalone R²")
```

### Step 2: Recursive Model Building

```{r recursive-models}
#| label: recursive-models

ordered_preds <- univariate_results$Predictor
bf_recursive <- basket_features %>% 
  dplyr::select(Date, return_next, c(ordered_preds)) %>% 
  tidyr::drop_na()

MultRegAuto <- function(data) {
  dependent    <- names(data)[2]
  independents <- names(data)[-1:-2]
  
  out <- dplyr::tibble(model = "init")
  
  for (i in seq_along(independents)) {
    betas      <- paste(independents[1:i], collapse = " + ")
    out[i, 1] <- glue::glue("{dependent} ~ {betas}")
  }
  
  out <- out %>% 
    dplyr::mutate(
      output = purrr::map(model, ~ stats::lm(as.formula(.x), data = data)),
      res    = purrr::map(output, broom::glance)
    ) %>% 
    tidyr::unnest(res) %>% 
    dplyr::select(model, r.squared, adj.r.squared, p.value) %>% 
    dplyr::mutate_if(is.numeric, round, 5)
  
  return(out)
}

recursive_raw <- MultRegAuto(bf_recursive)

recursive_results <- recursive_raw %>% 
  dplyr::mutate(
    Model = paste0("Model ", dplyr::row_number()),
    delta = adj.r.squared - dplyr::lag(adj.r.squared, default = NA_real_)
  ) %>% 
  dplyr::select(Model, r.squared, adj.r.squared, delta)

# Display table
recursive_results %>% 
  gt::gt() %>% 
  gt::fmt_percent(columns = c(r.squared, adj.r.squared, delta), decimals = 2) %>% 
  gt::tab_header(
    title    = "Step 2: Recursive Model Building",
    subtitle = "Adding predictors incrementally by univariate rank"
  )

# ================================================
# SELECT MODEL WITH MAX ADJ. R²
# ================================================

selected_row <- which.max(recursive_results$adj.r.squared)

# Selected predictors
selected_predictors <- ordered_preds[1:selected_row]


# VIF check
final_model <- lm(
  as.formula(recursive_raw$model[selected_row]), 
  data = bf_recursive
)

cat("VIF:\n")
print(round(car::vif(final_model), 2))
```

## 2.3 Tidymodels Workflow

### Step 1: Data Sampling (rsample)

```{r sampling}
#| label: sampling

df_model <- bf_clean
df_split <- rsample::initial_time_split(df_model, prop = 0.85)
df_train <- rsample::training(df_split)
df_test <- rsample::testing(df_split)

cat("Train:", nrow(df_train), "| Test:", nrow(df_test), "\n")
```

### Step 2: Feature Engineering (recipes)

```{r recipe}
#| label: recipe

# Use ONLY the predictors selected from recursive regression
selected_formula <- as.formula(
  paste("return_next ~", paste(selected_predictors, collapse = " + "))
)

cat("Selected Formula:\n")
print(selected_formula)
cat("\n")

recipe_pipeline <- recipes::recipe(selected_formula, data = df_train) %>%
  recipes::step_impute_median(all_numeric_predictors()) %>%
  recipes::step_normalize(all_numeric_predictors()) %>%
  recipes::step_zv(all_predictors()) %>%
  recipes::prep()

train_baked <- recipes::bake(recipe_pipeline, df_train)
test_baked <- recipes::bake(recipe_pipeline, df_test)

cat("Training set:", nrow(train_baked), "obs,", ncol(train_baked) - 1, "predictors\n")
cat("Test set:    ", nrow(test_baked), "obs,", ncol(test_baked) - 1, "predictors\n")
```

### Step 3: Modeling (parsnip)

#### Linear Regression

```{r model-lm}
#| label: model-lm

model_lm <- parsnip::linear_reg(mode = "regression") %>%
  parsnip::set_engine("lm") %>%
  parsnip::fit(return_next ~ ., data = train_baked)

parsnip::tidy(model_lm) %>%
  mutate(across(where(is.numeric), ~round(.x, 5))) %>%
  gt() %>%
  tab_header(title = "Linear Regression Coefficients")

parsnip::glance(model_lm) %>%
  select(r.squared, adj.r.squared, sigma, p.value) %>%
  gt() %>% fmt_number(decimals = 4) %>%
  tab_header(title = "In-Sample Model Fit")
```


### Step 4: Model Evaluation (yardstick)

```{r evaluation}
#| label: evaluation

lm_pred <- predict(model_lm, test_baked) %>% bind_cols(truth = test_baked$return_next)

lm_metrics <- lm_pred %>% yardstick::metrics(truth = truth, estimate = .pred)

bind_rows(
  lm_metrics %>% mutate(model = "Linear Regression"),
) %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  select(model, rmse, rsq, mae) %>%
  gt() %>%
  fmt_number(columns = c(rmse, mae), decimals = 5) %>%
  fmt_percent(columns = rsq, decimals = 2) %>%
  tab_header(title = "Out-of-Sample Performance")
```

### Predicted vs Actual

```{r pred-actual}
#| label: pred-actual

bind_rows(
  lm_pred %>% mutate(model = "Linear Regression"),
) %>%
  ggplot(aes(x = .pred, y = truth)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  facet_wrap(~model) +
  labs(title = "Predicted vs Actual Returns") +
  theme_minimal()
```

---

# Part 3: Card-Level Risk Analysis

**Beyond the basket:** Individual cards have vastly different risk profiles. What drives these differences?

## Calculate Card-Level Risk Metrics

```{r card-metrics}
#| label: card-metrics

basket_daily <- basket_index %>% select(Date, basket_return)

card_metrics <- df %>%
  left_join(basket_daily, by = "Date") %>%
  group_by(card_id, card_name, rarity) %>%
  summarise(
    n_obs = n(),
    mean_return = mean(daily_return, na.rm = TRUE) * 252,
    volatility = sd(daily_return, na.rm = TRUE),
    ann_volatility = sd(daily_return, na.rm = TRUE) * sqrt(252),
    avg_price = mean(Price, na.rm = TRUE),
    avg_volume = mean(Volume, na.rm = TRUE),
    beta = cov(daily_return, basket_return, use = "complete.obs") / 
           var(basket_return, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    rarity_numeric = case_when(
      rarity == "Common" ~ 1,
      rarity == "Uncommon" ~ 2,
      rarity == "Rare" ~ 3,
      rarity == "Holo Rare" ~ 4,
      rarity == "Promo" ~ 4.5,
      rarity == "Ultra Rare" ~ 5,
      rarity == "Secret Rare" ~ 6,
      rarity == "Double Rare" ~ 5.5,
      TRUE ~ 3
    ),
    sharpe = mean_return / ann_volatility
  )

card_metrics %>%
  arrange(desc(ann_volatility)) %>%
  select(card_name, rarity, ann_volatility, beta, avg_price) %>%
  head(12) %>%
  gt() %>%
  fmt_percent(columns = ann_volatility, decimals = 1) %>%
  fmt_number(columns = beta, decimals = 2) %>%
  fmt_currency(columns = avg_price, currency = "USD") %>%
  tab_header(title = "Most Volatile Cards", subtitle = "Ranked by annualized volatility")
```

## Volatility by Rarity

```{r vol-by-rarity}
#| label: vol-by-rarity

ggplot(card_metrics, aes(x = reorder(rarity, rarity_numeric), y = ann_volatility)) +
  geom_boxplot(aes(fill = rarity), alpha = 0.7) +
  geom_jitter(width = 0.2, alpha = 0.5, size = 2) +
  labs(
    title = "Card Volatility by Rarity Tier",
    subtitle = "Do rarer cards exhibit more price volatility?",
    x = "Rarity", y = "Annualized Volatility"
  ) +
  theme_minimal() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, hjust = 1))
```

## ML Approach: What Drives Card Volatility?

We apply the same tidymodels workflow to model card-level volatility.

```{r vol-ml-workflow}
#| label: vol-ml-workflow

vol_data <- card_metrics %>%
  select(ann_volatility, rarity_numeric, avg_price, avg_volume) %>%
  drop_na() %>%
  mutate(
    log_price = log(avg_price + 1),
    log_volume = log(avg_volume + 1)
  )

vol_split <- rsample::initial_split(vol_data, prop = 0.80)
vol_train <- rsample::training(vol_split)
vol_test <- rsample::testing(vol_split)

cat("Volatility model - Training:", nrow(vol_train), "| Testing:", nrow(vol_test), "\n")

vol_recipe <- recipes::recipe(
  ann_volatility ~ rarity_numeric + log_price + log_volume,
  data = vol_train
) %>%
  recipes::step_normalize(all_numeric_predictors()) %>%
  recipes::prep()

vol_train_baked <- recipes::bake(vol_recipe, vol_train)
vol_test_baked <- recipes::bake(vol_recipe, vol_test)

vol_model_lm <- parsnip::linear_reg(mode = "regression") %>%
  parsnip::set_engine("lm") %>%
  parsnip::fit(ann_volatility ~ ., data = vol_train_baked)

parsnip::tidy(vol_model_lm) %>%
  mutate(across(where(is.numeric), ~round(.x, 4))) %>%
  gt() %>%
  tab_header(title = "Volatility Model Coefficients")

vol_model_lm %>%
  stats::predict(new_data = vol_test_baked) %>%
  dplyr::bind_cols(truth = vol_test_baked$ann_volatility) %>%
  yardstick::metrics(truth = truth, estimate = .pred) %>%
  gt() %>%
  fmt_number(columns = .estimate, decimals = 4) %>%
  tab_header(title = "Volatility Model: Out-of-Sample Performance")
```

## Decision Tree: Volatility Segmentation

```{r vol-tree}
#| label: vol-tree

vol_tree <- parsnip::decision_tree(mode = "regression") %>%
  parsnip::set_engine("rpart") %>%
  parsnip::fit(ann_volatility ~ rarity_numeric + avg_price + avg_volume, 
               data = card_metrics %>% drop_na())

rpart.plot(
  vol_tree$fit,
  roundint = FALSE,
  cex = 0.9,
  fallen.leaves = TRUE,
  extra = "auto",
  main = "What Drives Card Volatility?",
  box.palette = "Blue"
)
```

**Commentary:** The decision tree segments cards into risk buckets. The first split identifies the most important volatility driver. Terminal nodes show average volatility for each segment—useful for categorizing new cards.

## Summary by Rarity

```{r summary-rarity}
#| label: summary-rarity

card_metrics %>%
  group_by(rarity) %>%
  summarise(
    n_cards = n(),
    avg_volatility = mean(ann_volatility, na.rm = TRUE),
    avg_beta = mean(beta, na.rm = TRUE),
    avg_price = mean(avg_price, na.rm = TRUE),
    avg_sharpe = mean(sharpe, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(avg_volatility)) %>%
  gt() %>%
  fmt_percent(columns = avg_volatility, decimals = 1) %>%
  fmt_number(columns = c(avg_beta, avg_sharpe), decimals = 2) %>%
  fmt_currency(columns = avg_price, currency = "USD") %>%
  tab_header(
    title = "Risk-Return Profile by Rarity",
    subtitle = "Which rarity tiers offer best risk-adjusted returns?"
  )
```

---

# Part 4: Individual Card Backtest (Card 6)

Does the basket model generalize to individual cards? We test on Umbreon VMax Alternate Art (highly traded card) and Ash and Pikachu (low volume card).

## Extract Umbreon Data

```{r high_volume_data}
hv_data <- df %>%
  filter(card_id == 76, Condition == "Near Mint") %>%
  arrange(Date) %>%
  select(Date, Price, daily_return, card_name, set_name, rarity, DataType) %>%
  drop_na(daily_return)

hv_data_actual <- hv_data 
  # filter(DataType == "Actual Sale")  # ← KEY FILTER

cat("\n=== After Filtering to Actual Sales ===\n")
cat("Actual sale observations:", nrow(hv_data_actual), "\n")

cat("=== High Value ===\n")
cat("Card Name:", unique(hv_data$card_name)[1], "\n")
cat("Set:", unique(hv_data$set_name)[1], "\n")
cat("Observations:", nrow(hv_data), "\n")
```

## Feature Engineering

```{r high_volume_features}
hv_features <- hv_data_actual %>%
  left_join(sentiment_daily, by = "Date") %>%
  left_join(spy_daily, by = "Date") %>%
  mutate(
    return_next = lead(daily_return),
    return_lag1 = lag(daily_return, 1),
    return_lag2 = lag(daily_return, 2),
    return_lag3 = lag(daily_return, 3),
    return_5d = dplyr::lag(
      slider::slide_dbl(
        daily_return,
        ~ sum(.x),         
        .before   = 4,      
        .complete = TRUE    
      ),
      1                     
    ),
    vol_5d = dplyr::lag(
      slider::slide_dbl(
        daily_return,
        ~ stats::sd(.x),    
        .before   = 4,
        .complete = TRUE
      ),
      1
    ),
    sentiment_lag1 = lag(sentiment_index, 1),
    spy_lag1 = lag(spy_return, 1),
    spy_5d = dplyr::lag(
      slider::slide_dbl(
        spy_return,
        ~ sum(.x),
        .before   = 4,
        .complete = TRUE
      ),
      1
    ),
    dow = wday(Date, label = TRUE),
    is_holiday_season = ifelse(month(Date) %in% c(11, 12), 1, 0)
  ) %>%
  filter(!is.na(return_next), !is.na(vol_5d), !is.na(spy_lag1))

cat("High Value features:", nrow(hv_features), "observations\n")
```

## Train/Test Split & Apply Model

```{r high_volume_model}
hv_split <- rsample::initial_time_split(hv_features, prop = 0.70)
hv_train <- rsample::training(hv_split)
hv_test <- rsample::testing(hv_split)

cat("High Value - Train:", nrow(hv_train), "| Test:", nrow(hv_test), "\n")

hv_recipe <- recipes::recipe(
  as.formula(paste("return_next ~", paste(selected_predictors, collapse = " + "))),
  data = hv_train
) %>%
  recipes::step_impute_median(all_numeric_predictors()) %>%
  recipes::step_normalize(all_numeric_predictors()) %>%
  recipes::step_zv(all_predictors()) %>%
  recipes::prep()

hv_test_baked <- recipes::bake(hv_recipe, hv_test)
```

## Performance Comparison

```{r high_volume_performance}
hv_pred <- predict(model_lm, hv_test_baked) %>%
  bind_cols(truth = hv_test_baked$return_next)

hv_metrics <- hv_pred %>%
  yardstick::metrics(truth = truth, estimate = .pred)

hv_metrics %>%
  gt() %>%
  fmt_number(columns = .estimate, decimals = 4) %>%
  tab_header(title = "High Value: Out-of-Sample Performance")

hv_rsq <- hv_metrics %>% filter(.metric == "rsq") %>% pull(.estimate)
basket_rsq <- lm_metrics %>% filter(.metric == "rsq") %>% pull(.estimate)

cat("\n=== COMPARISON ===\n")
cat("Card 6 R²:  ", round(hv_rsq * 100, 2), "%\n")
cat("Basket R²:  ", round(basket_rsq * 100, 2), "%\n")
```
# Trading Simulation

```{r high_volume_simulation}
hv_trades <- hv_pred %>%
  bind_cols(hv_features %>% slice_tail(n = nrow(hv_test)) %>% select(Date, Price)) %>%
  mutate(
    signal = case_when(
      .pred > 0.01 ~ "BUY",
      .pred < -0.01 ~ "SELL",
      TRUE ~ "HOLD"
    ),
    strategy_return = case_when(
      signal == "BUY" ~ truth,
      signal == "SELL" ~ -truth,
      TRUE ~ 0
    ),
    bnh_return = truth,
    cum_strategy = cumprod(1 + replace_na(strategy_return, 0)),
    cum_bnh = cumprod(1 + replace_na(bnh_return, 0))
  )

strategy_performance_hv <- hv_trades %>%
  summarise(
    n_trades = sum(signal != "HOLD"),
    total_return_strategy = last(cum_strategy) - 1,
    total_return_bnh = last(cum_bnh) - 1,
    outperformance = (last(cum_strategy) - 1) - (last(cum_bnh) - 1)
  )

cat("\n=== TRADING SIMULATION ===\n")
cat("Number of trades:    ", strategy_performance_hv$n_trades, "\n")
cat("Strategy return:     ", round(strategy_performance_hv$total_return_strategy * 100, 2), "%\n")
cat("Buy-and-hold return: ", round(strategy_performance_hv$total_return_bnh * 100, 2), "%\n")
cat("Outperformance:      ", round(strategy_performance_hv$outperformance * 100, 2), "pp\n")
```

```{r high_volume_plot}
ggplot(hv_trades, aes(x = Date)) +
  geom_line(aes(y = cum_strategy, color = "ML Strategy"), linewidth = 1.2) +
  geom_line(aes(y = cum_bnh, color = "Buy & Hold"), linewidth = 1, alpha = 0.7) +
  geom_hline(yintercept = 1, linetype = "dashed", alpha = 0.5) +
  scale_color_manual(values = c("ML Strategy" = "#e74c3c", "Buy & Hold" = "#95a5a6")) +
  labs(
    title = paste("Card 6:", unique(hv_data$card_name)[1]),
    subtitle = "ML Strategy vs Buy-and-Hold",
    x = "Date", y = "Cumulative Return"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## Extract Ash & Pikachu Data

```{r card6-data}
#| label: card7-data

card_7_data <- df %>%
  filter(card_id == 1, Condition == "Near Mint") %>%
  arrange(Date) %>%
  select(Date, Price, daily_return, card_name, set_name, rarity, DataType) %>%
  drop_na(daily_return)

card_7_data_actual <- card_7_data 
  # filter(DataType == "Actual Sale")  # ← KEY FILTER

cat("\n=== After Filtering to Actual Sales ===\n")
cat("Actual sale observations:", nrow(card_7_data_actual), "\n")
cat("Removed interpolated:", nrow(card_7_data) - nrow(card_7_data_actual), "observations\n")

cat("=== CARD 6 INFO ===\n")
cat("Card Name:", unique(card_7_data$card_name)[1], "\n")
cat("Set:", unique(card_7_data$set_name)[1], "\n")
cat("Observations:", nrow(card_7_data), "\n")
```

## Feature Engineering

```{r card6-features}
#| label: card7-features

card_7_features <- card_7_data_actual %>%
  left_join(sentiment_daily, by = "Date") %>%
  left_join(spy_daily, by = "Date") %>%
  mutate(
    return_next = lead(daily_return),
    return_lag1 = lag(daily_return, 1),
    return_lag2 = lag(daily_return, 2),
    return_lag3 = lag(daily_return, 3),
    return_5d = dplyr::lag(
      slider::slide_dbl(
        daily_return,
        ~ sum(.x),         
        .before   = 4,      
        .complete = TRUE    
      ),
      1                     
    ),
    vol_5d = dplyr::lag(
      slider::slide_dbl(
        daily_return,
        ~ stats::sd(.x),    
        .before   = 4,
        .complete = TRUE
      ),
      1
    ),
    sentiment_lag1 = lag(sentiment_index, 1),
    spy_lag1 = lag(spy_return, 1),
    spy_5d = dplyr::lag(
      slider::slide_dbl(
        spy_return,
        ~ sum(.x),
        .before   = 4,
        .complete = TRUE
      ),
      1
    ),
    dow = wday(Date, label = TRUE),
    is_holiday_season = ifelse(month(Date) %in% c(11, 12), 1, 0)
  ) %>%
  filter(!is.na(return_next), !is.na(vol_5d), !is.na(spy_lag1))

cat("Card 6 features:", nrow(card_7_features), "observations\n")
```

## Train/Test Split & Apply Model

```{r card6-model}
#| label: card6-model

card7_split <- rsample::initial_time_split(card_7_features, prop = 0.70)
card7_train <- rsample::training(card7_split)
card7_test <- rsample::testing(card7_split)

cat("Card 7 - Train:", nrow(card7_train), "| Test:", nrow(card7_test), "\n")

card7_recipe <- recipes::recipe(
  as.formula(paste("return_next ~", paste(selected_predictors, collapse = " + "))),
  data = card7_train
) %>%
  recipes::step_impute_median(all_numeric_predictors()) %>%
  recipes::step_normalize(all_numeric_predictors()) %>%
  recipes::step_zv(all_predictors()) %>%
  recipes::prep()

card7_test_baked <- recipes::bake(card7_recipe, card7_test)
```

## Performance Comparison

```{r card6-performance}
#| label: card6-performance

card7_pred <- predict(model_lm, card7_test_baked) %>%
  bind_cols(truth = card7_test_baked$return_next)

card7_metrics <- card7_pred %>%
  yardstick::metrics(truth = truth, estimate = .pred)

card7_metrics %>%
  gt() %>%
  fmt_number(columns = .estimate, decimals = 4) %>%
  tab_header(title = "Card 7: Out-of-Sample Performance")

card7_rsq <- card7_metrics %>% filter(.metric == "rsq") %>% pull(.estimate)
basket_rsq <- lm_metrics %>% filter(.metric == "rsq") %>% pull(.estimate)

cat("\n=== COMPARISON ===\n")
cat("Card 7 R²:  ", round(card7_rsq * 100, 2), "%\n")
cat("Basket R²:  ", round(basket_rsq * 100, 2), "%\n")
```

## Trading Simulation

```{r card6-trading}
#| label: card7-trading

card7_trades <- card7_pred %>%
  bind_cols(card_7_features %>% slice_tail(n = nrow(card7_test)) %>% select(Date, Price)) %>%
  mutate(
    signal = case_when(
      .pred > 0.01 ~ "BUY",
      .pred < -0.01 ~ "SELL",
      TRUE ~ "HOLD"
    ),
    strategy_return = case_when(
      signal == "BUY" ~ truth,
      signal == "SELL" ~ -truth,
      TRUE ~ 0
    ),
    bnh_return = truth,
    cum_strategy = cumprod(1 + replace_na(strategy_return, 0)),
    cum_bnh = cumprod(1 + replace_na(bnh_return, 0))
  )

strategy_performance <- card7_trades %>%
  summarise(
    n_trades = sum(signal != "HOLD"),
    total_return_strategy = last(cum_strategy) - 1,
    total_return_bnh = last(cum_bnh) - 1,
    outperformance = (last(cum_strategy) - 1) - (last(cum_bnh) - 1)
  )

cat("\n=== TRADING SIMULATION ===\n")
cat("Number of trades:    ", strategy_performance$n_trades, "\n")
cat("Strategy return:     ", round(strategy_performance$total_return_strategy * 100, 2), "%\n")
cat("Buy-and-hold return: ", round(strategy_performance$total_return_bnh * 100, 2), "%\n")
cat("Outperformance:      ", round(strategy_performance$outperformance * 100, 2), "pp\n")
```

```{r card7-plot}
#| label: card7-plot

ggplot(card7_trades, aes(x = Date)) +
  geom_line(aes(y = cum_strategy, color = "ML Strategy"), linewidth = 1.2) +
  geom_line(aes(y = cum_bnh, color = "Buy & Hold"), linewidth = 1, alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) +
  scale_color_manual(values = c("ML Strategy" = "#e74c3c", "Buy & Hold" = "#95a5a6")) +
  labs(
    title = paste("Card 7:", unique(card_7_data$card_name)[1]),
    subtitle = "ML Strategy vs Buy-and-Hold",
    x = "Date", y = "Cumulative Return"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

---

# Conclusions & Key Findings

## 1. Index Behavior

```{r conclusion-1}
#| label: conclusion-1

cat("Fat Tails (Kurtosis):", round(e1071::kurtosis(returns, na.rm = TRUE), 2), 
    ifelse(e1071::kurtosis(returns, na.rm = TRUE) > 3, "→ YES\n", "→ NO\n"))
cat("Skewness:", round(e1071::skewness(returns, na.rm = TRUE), 2),
    ifelse(e1071::skewness(returns, na.rm = TRUE) < 0, "→ Crash-prone\n", "→ Rally-prone\n"))
cat("Annualized Volatility:", round(sd(returns, na.rm = TRUE) * sqrt(252) * 100, 1), "%\n")
```

## 2. Predictability (ML Results)

The tidymodels workflow revealed:

- **Best predictors** from recursive screening: `r paste(selected_predictors, collapse = ", ")`
- **Out-of-sample R²**: Low, as expected for financial returns
- **Direction accuracy**: [Check results above]

## 3. Risk Factors

Key volatility drivers from the ML analysis:

- **Rarity effect**: See decision tree for segmentation
- **Price level effect**: Higher-priced cards [more/less] volatile
- **Volume effect**: Trading activity [does/doesn't] affect risk

## Limitations

1. **Small sample size**: Only ~76 actual trading days after filtering interpolated data
2. **Single condition**: Near Mint only
3. **Limited history**: ~1 year of data
4. **Survivorship bias**: Only actively traded cards included
5. **Transaction costs**: Not considered in backtest

## Workflow Summary

```
DATA CLEANING              Ch. 8: Recursive           Ch. 11: Tidymodels
─────────────              ─────────────────          ──────────────────
Filter interpolated        Univariate Screening       rsample::split()
Keep Volume > 0                   ↓                          ↓
       ↓                   Incremental Building       recipes::recipe()
Remove fake autocorr              ↓                          ↓
                           VIF Diagnostics            parsnip::fit()
                                  ↓                          ↓
                           SELECT FEATURES ─────────→ yardstick::metrics()
```

---

# Session Info

```{r session-info}
sessionInfo()
```
